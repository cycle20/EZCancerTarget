#
# main.yml: CI workflow
#

name: CI
env:
  ARTIFACT_01: clue.R.tgz
  ARTIFACT_02: dataPatch.R.tgz
  ARTIFACT_03: renderWebPage.R.tgz
  UNIPROT_CACHE_DIR: G.D.CACHE

# Controls when the action will run. 
on:
  # Triggers the workflow on push or pull request events but only for the main branch
  push:
    branches: [ target ]
  pull_request:
    branches: [ main ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:

  # JOB: CLUE
  clue:
    name: "Download perturbagens from clue.io (R/clue.R)"
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:

      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      - name: Download recent "${{ env.ARTIFACT_01 }}"
        run: |
          bash -x src/get_artifact.bash "${{ secrets.GITHUB_TOKEN }}" "${{ env.ARTIFACT_01 }}"
          rm "${{ env.ARTIFACT_01 }}" # TODO: should be controlled by workflow parameter
        continue-on-error: true

      #
      # This step inspired by this comment: https://github.com/actions/toolkit/issues/299#issuecomment-574918750
      #
      # Is there a cached content?
      # If yes, then clue.io is not needed.
      - name: Previous result exists
        id: cache_exists
        run: |
          if [ -f "${{ env.ARTIFACT_01 }}" ]; then
            echo ::set-output name=cache_check::yes
          fi

      #
      # Dependencies
      #
      - name: "Dependencies: R pkgs from added PPA and CRAN"
        if: steps.cache_exists.outputs.cache_check != 'yes'
        run: sudo bash src/gh_action__setup_and_install.bash

      # Exec clue.R
      - name: R/clue.R
        if: steps.cache_exists.outputs.cache_check != 'yes'
        run: Rscript R/clue.R
        env:
          CLUE_USER_KEY: ${{ secrets.CLUE_USER_KEY }}
          SERVICE_TOKEN_JSON: ${{ secrets.CLUE_GS4_SERVICE_TOKEN_JSON }}
      
      # Wrap OUTPUT directory
      - name: Create archive file for "caching"
        if: steps.cache_exists.outputs.cache_check != 'yes'
        run: bash src/wrap_directory.bash "${{ env.ARTIFACT_01 }}" OUTPUT

      # Caching
      - name: Upload result artifact as a cache custom cache solution
        if: steps.cache_exists.outputs.cache_check != 'yes'
        uses: actions/upload-artifact@v2.2.3
        with:
          name: "${{ env.ARTIFACT_01 }}"
          path: "${{ env.ARTIFACT_01 }}"
          retention-days: 1

      - run: md5sum "${{ env.ARTIFACT_01 }}"






  # JOB: DATAPATCH
  # Supply missing data fields in dataset resulted by R/clue.R
  dataPatch:
    name: "Collect missing data (R/dataPatch.R)"
    runs-on: ubuntu-latest
    needs: [clue]

    steps:

      # Repository checkout
      - uses: actions/checkout@v2

      - name: Environment
        run: env | sort -u


      # Cache restore: artifact of "clue" job
      - name: "Restore 'cache' of ${{ env.ARTIFACT_01 }}"
        run: |
          bash -x src/get_artifact.bash "${{ secrets.GITHUB_TOKEN }}" "${{ env.ARTIFACT_01 }}"
          ls -lt PREV_RESULT
          mv PREV_RESULT OUTPUT

      # Cache restore of pervious (it is optional in case of dataPatch)
      - name: "Cache: try to get the most recent '${{ env.ARTIFACT_02 }}' artifact"
        continue-on-error: true
        run: |
          bash -x src/get_artifact.bash "${{ secrets.GITHUB_TOKEN }}" "${{ env.ARTIFACT_02 }}"

      # Cache restore 02/b: UniProt cache (it is needed for correct subcellular images)
      - name: "Drive task: download external UniProt cache stuff"
        run: |
          echo "Download cache stuff..."
          bash src/google_download.bash "${{ secrets.UNIPROT_PRE_CACHED }}" u.tgz
          echo "md5sum of cache file"
          md5sum u.tgz

      - name: tarball verbosed extraction of UniProt cache stuffs into ${{ env.UNIPROT_CACHE_DIR }}
        run: |
          mkdir -p "${{ env.UNIPROT_CACHE_DIR }}"
          tar --directory="${{ env.UNIPROT_CACHE_DIR }}" -xzf u.tgz

          mkdir -p OUTPUT
          tar --directory=OUTPUT -xzf ${{ env.ARTIFACT_02 }}

      - name: Merge UniProt cache into restored OUTPUT directory
        run: |
          UNI_CACHE="${{ env.UNIPROT_CACHE_DIR }}"
          GH_CACHE="OUTPUT/DATAPATH_CACHE"
          bash -x src/gh_action__merge_caches.bash "$UNI_CACHE" "$GH_CACHE"




      # Install dependencies
      - name: "Install dependencies: R pkgs from added PPA and CRAN"
        run: sudo bash src/gh_action__setup_and_install.bash




      # Exec dataPatch.R
      # TODO: this can be a long process: downloads in parallel jobs is a potential improvement
      - name: R/dataPatch.R
        run: Rscript R/dataPatch.R

      # Wrap OUTPUT into tar package
      - name: Prepare archive file for upload
        run: |
          bash src/wrap_directory.bash "${{ env.ARTIFACT_02 }}" OUTPUT
          md5sum "${{ env.ARTIFACT_02 }}"

      # Caching
      - name: Upload result
        uses: actions/upload-artifact@v2.2.3
        with:
          name: "${{ env.ARTIFACT_02 }}"
          path: "${{ env.ARTIFACT_02 }}"


  # experimental job to practice GHA features
  render:
    name: Render web representation of current dataset
    # The type of runner that the job will run on
    runs-on: ubuntu-latest
    needs: [dataPatch]

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:

      - uses: actions/checkout@v2
  
      - name: Environment
        run: env | sort -u


      - name: Get most recent "${{ env.ARTIFACT_02 }}" artifact
        run: |
          bash -x src/get_artifact.bash "${{ secrets.GITHUB_TOKEN }}" "${{ env.ARTIFACT_02 }}"
  
      - name: "Extract ${{ env.ARTIFACT_02 }}"
        run: |
          echo "md5sum of cache file"
          md5sum ${{ env.ARTIFACT_02 }}
          mkdir -p OUTPUT
          tar --directory=OUTPUT -xzf ${{ env.ARTIFACT_02 }}
  
      - name: "Setup R dependencies"
        run: |
          sudo bash src/gh_action__setup_and_install.bash
  
      - name: Exec page rendering (renderWebPage.R)
        run: |
          Rscript R/renderWebPage.R
          ls -lt OUTPUT/*.html
  
      - name: "Wrap pages into '${{ env.ARTIFACT_03 }}'"
        run: |
          tar --directory=OUTPUT -cvzf "${{ env.ARTIFACT_03 }}" \
            index.target.with.data.html index.target.no_data.html
  
      - name: "Upload artifact '${{ env.ARTIFACT_03 }}'"
        uses: actions/upload-artifact@v2.2.3
        with:
          name: "${{ env.ARTIFACT_03 }}"
          path: "${{ env.ARTIFACT_03 }}"
          retention-days: 5






  deployPages:
    name: Commit new pages from artifact of render job
    runs-on: ubuntu-latest
    needs: [render]

    steps:

      - uses: actions/checkout@v2
        with:
          ref: gh-pages

      - name: Environment
        run: env | sort -u

      - name: get artifact "${{ env.ARTIFACT_03 }}"
        run: |
          tree -d
          # get the script file
          curl -o get_artifact.bash  https://raw.githubusercontent.com/cycle20/scancer/main/src/get_artifact.bash
 
          bash get_artifact.bash "${{ secrets.GITHUB_TOKEN }}" "${{ env.ARTIFACT_03 }}"
          # drop the script file (from the index as well)
          rm get_artifact.bash
          tree -d

      - name: gh-pages "deploy"
        run: |
          git config  --local user.email "46718432+cycle20@users.noreply.github.com"
          git config  --local user.name cycle20

          mv PREV_RESULT/index.target.with.data.html .
          git add index.target.with.data.html
          git commit -m "Test push from workflow"

          git push "https://${{ secrets.GH_PAGES_PAT }}@github.com/cycle20/scancer.git" \
            gh-pages:gh-pages
          git log -1
          git status
